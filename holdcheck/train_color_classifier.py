"""
ğŸ¤– ê²½ëŸ‰ ìƒ‰ìƒ ë¶„ë¥˜ AI ëª¨ë¸ í•™ìŠµ
CLIPë³´ë‹¤ í›¨ì”¬ ë¹ ë¥´ê³  ê°€ë²¼ìš´ ì „ìš© ëª¨ë¸ (~1-5MB)

3ê°€ì§€ ëª¨ë¸ ì œê³µ:
1. KNN (ê°€ì¥ ë¹ ë¦„, ì‹¤ì‹œê°„)
2. SVM (ì •í™•ë„ ì¤‘ê°„)
3. ì‘ì€ ì‹ ê²½ë§ (ê°€ì¥ ì •í™•, ì•½ê°„ ëŠë¦¼)
"""

import json
import numpy as np
import pickle
from pathlib import Path


def load_feedback_dataset(dataset_path="holdcheck/color_feedback_dataset.json"):
    """í”¼ë“œë°± ë°ì´í„°ì…‹ ë¡œë“œ"""
    if not Path(dataset_path).exists():
        print(f"âŒ ë°ì´í„°ì…‹ ì—†ìŒ: {dataset_path}")
        print("   ë¨¼ì € í”¼ë“œë°± UIì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”!")
        return None
    
    with open(dataset_path, 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    
    print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ: {dataset['total_samples']}ê°œ ìƒ˜í”Œ")
    return dataset


def prepare_training_data(dataset):
    """í•™ìŠµ ë°ì´í„° ì¤€ë¹„"""
    X = []  # íŠ¹ì§• (RGB, HSV, í†µê³„ ë“±)
    y = []  # ë¼ë²¨ (ìƒ‰ìƒ ì´ë¦„)
    
    for sample in dataset['samples']:
        # íŠ¹ì§• ì¶”ì¶œ: RGB + HSV
        rgb = sample.get('rgb', [0, 0, 0])
        hsv = sample.get('hsv', [0, 0, 0])
        
        # ì¶”ê°€ íŠ¹ì§• ê³„ì‚°
        r, g, b = rgb
        h, s, v = hsv
        
        # ìƒ‰ìƒ íŠ¹ì§• ë²¡í„° (12ì°¨ì›)
        features = [
            r, g, b,  # RGB (3)
            h, s, v,  # HSV (3)
            r / 255.0, g / 255.0, b / 255.0,  # ì •ê·œí™” RGB (3)
            s / 255.0, v / 255.0,  # ì •ê·œí™” SV (2)
            max(r, g, b) - min(r, g, b),  # ìƒ‰ìƒ ë²”ìœ„ (1)
        ]
        
        X.append(features)
        y.append(sample['correct_color'])
    
    return np.array(X), np.array(y)


def train_knn_model(X, y, n_neighbors=5):
    """KNN ëª¨ë¸ í•™ìŠµ (ê°€ì¥ ë¹ ë¦„)"""
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.preprocessing import StandardScaler
    
    print(f"\nğŸš€ KNN ëª¨ë¸ í•™ìŠµ ì¤‘... (k={n_neighbors})")
    
    # ì •ê·œí™”
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # KNN í•™ìŠµ
    model = KNeighborsClassifier(n_neighbors=n_neighbors, weights='distance')
    model.fit(X_scaled, y)
    
    print(f"âœ… KNN í•™ìŠµ ì™„ë£Œ!")
    
    return model, scaler


def train_svm_model(X, y):
    """SVM ëª¨ë¸ í•™ìŠµ (ì •í™•ë„ ì¤‘ê°„)"""
    from sklearn.svm import SVC
    from sklearn.preprocessing import StandardScaler
    
    print(f"\nğŸš€ SVM ëª¨ë¸ í•™ìŠµ ì¤‘...")
    
    # ì •ê·œí™”
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # SVM í•™ìŠµ
    model = SVC(kernel='rbf', probability=True, gamma='auto')
    model.fit(X_scaled, y)
    
    print(f"âœ… SVM í•™ìŠµ ì™„ë£Œ!")
    
    return model, scaler


def train_neural_network_model(X, y, epochs=100):
    """ì‘ì€ ì‹ ê²½ë§ í•™ìŠµ (ê°€ì¥ ì •í™•)"""
    from sklearn.preprocessing import StandardScaler, LabelEncoder
    from sklearn.model_selection import train_test_split
    import tensorflow as tf
    from tensorflow import keras
    
    print(f"\nğŸš€ ì‹ ê²½ë§ ëª¨ë¸ í•™ìŠµ ì¤‘... (epochs={epochs})")
    
    # ì •ê·œí™”
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # ë¼ë²¨ ì¸ì½”ë”©
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    
    # ë°ì´í„° ë¶„í• 
    X_train, X_val, y_train, y_val = train_test_split(
        X_scaled, y_encoded, test_size=0.2, random_state=42
    )
    
    # ì‹ ê²½ë§ êµ¬ì¡° (ë§¤ìš° ì‘ìŒ, ~100KB)
    model = keras.Sequential([
        keras.layers.Dense(32, activation='relu', input_shape=(X.shape[1],)),
        keras.layers.Dropout(0.3),
        keras.layers.Dense(16, activation='relu'),
        keras.layers.Dropout(0.2),
        keras.layers.Dense(len(np.unique(y)), activation='softmax')
    ])
    
    # ì»´íŒŒì¼
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # í•™ìŠµ
    history = model.fit(
        X_train, y_train,
        epochs=epochs,
        batch_size=32,
        validation_data=(X_val, y_val),
        verbose=0
    )
    
    # ìµœì¢… ì •í™•ë„
    val_accuracy = history.history['val_accuracy'][-1]
    print(f"âœ… ì‹ ê²½ë§ í•™ìŠµ ì™„ë£Œ! (ê²€ì¦ ì •í™•ë„: {val_accuracy:.1%})")
    
    return model, scaler, label_encoder


def save_model(model, scaler, model_type, label_encoder=None, 
               output_dir="holdcheck/models"):
    """ëª¨ë¸ ì €ì¥"""
    import os
    os.makedirs(output_dir, exist_ok=True)
    
    if model_type == "neural_network":
        # TensorFlow ëª¨ë¸ ì €ì¥
        model.save(f"{output_dir}/color_classifier_nn.h5")
        
        # ìŠ¤ì¼€ì¼ëŸ¬ì™€ ë¼ë²¨ ì¸ì½”ë” ì €ì¥
        with open(f"{output_dir}/color_classifier_nn_scaler.pkl", 'wb') as f:
            pickle.dump(scaler, f)
        with open(f"{output_dir}/color_classifier_nn_labels.pkl", 'wb') as f:
            pickle.dump(label_encoder, f)
        
        print(f"ğŸ’¾ ì‹ ê²½ë§ ëª¨ë¸ ì €ì¥: {output_dir}/color_classifier_nn.h5")
    
    else:
        # Scikit-learn ëª¨ë¸ ì €ì¥
        model_data = {
            'model': model,
            'scaler': scaler,
            'model_type': model_type
        }
        
        output_path = f"{output_dir}/color_classifier_{model_type}.pkl"
        with open(output_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"ğŸ’¾ {model_type.upper()} ëª¨ë¸ ì €ì¥: {output_path}")


def load_trained_model(model_type="knn", model_dir="holdcheck/models"):
    """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
    if model_type == "neural_network":
        import tensorflow as tf
        from tensorflow import keras
        
        model = keras.models.load_model(f"{model_dir}/color_classifier_nn.h5")
        
        with open(f"{model_dir}/color_classifier_nn_scaler.pkl", 'rb') as f:
            scaler = pickle.load(f)
        with open(f"{model_dir}/color_classifier_nn_labels.pkl", 'rb') as f:
            label_encoder = pickle.load(f)
        
        return model, scaler, label_encoder
    
    else:
        model_path = f"{model_dir}/color_classifier_{model_type}.pkl"
        with open(model_path, 'rb') as f:
            model_data = pickle.load(f)
        
        return model_data['model'], model_data['scaler'], None


def predict_color(model, scaler, rgb, hsv, label_encoder=None, model_type="knn"):
    """ìƒ‰ìƒ ì˜ˆì¸¡"""
    r, g, b = rgb
    h, s, v = hsv
    
    # íŠ¹ì§• ì¶”ì¶œ
    features = np.array([[
        r, g, b,
        h, s, v,
        r / 255.0, g / 255.0, b / 255.0,
        s / 255.0, v / 255.0,
        max(r, g, b) - min(r, g, b),
    ]])
    
    # ì •ê·œí™”
    features_scaled = scaler.transform(features)
    
    # ì˜ˆì¸¡
    if model_type == "neural_network":
        probs = model.predict(features_scaled, verbose=0)[0]
        pred_idx = np.argmax(probs)
        confidence = float(probs[pred_idx])
        color_name = label_encoder.inverse_transform([pred_idx])[0]
    else:
        color_name = model.predict(features_scaled)[0]
        
        # ì‹ ë¢°ë„ ê³„ì‚° (í™•ë¥  ì§€ì›í•˜ëŠ” ê²½ìš°)
        if hasattr(model, 'predict_proba'):
            probs = model.predict_proba(features_scaled)[0]
            confidence = float(np.max(probs))
        else:
            confidence = 0.85  # ê¸°ë³¸ê°’
    
    return color_name, confidence


def ml_based_color_clustering(hold_data, vectors, model_type="knn", 
                              model_dir="holdcheck/models"):
    """
    ğŸ¤– ML ëª¨ë¸ ê¸°ë°˜ ìƒ‰ìƒ í´ëŸ¬ìŠ¤í„°ë§
    
    ë£° ê¸°ë°˜ë³´ë‹¤ ì •í™•í•˜ê³ , CLIPë³´ë‹¤ ë¹ ë¦„!
    
    Args:
        hold_data: í™€ë“œ ë°ì´í„°
        vectors: ì‚¬ìš© ì•ˆ í•¨
        model_type: "knn", "svm", "neural_network"
        model_dir: ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
    
    Returns:
        hold_data: ê·¸ë£¹ ì •ë³´ê°€ ì¶”ê°€ëœ í™€ë“œ ë°ì´í„°
    """
    import time
    import cv2
    
    if len(hold_data) == 0:
        return hold_data
    
    start_time = time.time()
    
    print(f"\nğŸ¤– ML ëª¨ë¸ ê¸°ë°˜ ìƒ‰ìƒ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘")
    print(f"   ëª¨ë¸: {model_type.upper()}")
    print(f"   í™€ë“œ ê°œìˆ˜: {len(hold_data)}ê°œ")
    
    # ëª¨ë¸ ë¡œë“œ
    try:
        model, scaler, label_encoder = load_trained_model(model_type, model_dir)
    except FileNotFoundError:
        print(f"âŒ ëª¨ë¸ ì—†ìŒ: {model_dir}/color_classifier_{model_type}")
        print("   ë¨¼ì € train_color_classifier.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”!")
        return hold_data
    
    # ê° í™€ë“œ ë¶„ë¥˜
    color_groups = {}
    
    for hold_idx, hold in enumerate(hold_data):
        # RGB/HSV ê°€ì ¸ì˜¤ê¸°
        if "dominant_hsv" in hold:
            h, s, v = hold["dominant_hsv"]
        else:
            h, s, v = 0, 0, 128
        
        if "dominant_rgb" in hold:
            rgb = hold["dominant_rgb"]
        else:
            hsv_arr = np.uint8([[[h, s, v]]])
            rgb_arr = cv2.cvtColor(hsv_arr, cv2.COLOR_HSV2RGB)[0][0]
            rgb = rgb_arr.tolist()
        
        # ì˜ˆì¸¡
        color_name, confidence = predict_color(
            model, scaler, rgb, [h, s, v], 
            label_encoder, model_type
        )
        
        # í™€ë“œì— ì •ë³´ ì¶”ê°€
        hold["clip_color_name"] = color_name
        hold["clip_confidence"] = confidence
        hold["color_method"] = f"ml_{model_type}"
        
        # ê·¸ë£¹í•‘
        if color_name not in color_groups:
            color_groups[color_name] = []
        color_groups[color_name].append(hold)
    
    # ê·¸ë£¹ ID í• ë‹¹
    color_order = ["black", "white", "gray", "red", "orange", "yellow", 
                   "green", "mint", "blue", "purple", "pink", "brown", "unknown"]
    
    group_idx = 0
    for color_name in color_order:
        if color_name in color_groups:
            for hold in color_groups[color_name]:
                hold["group"] = f"g{group_idx}"
            group_idx += 1
    
    elapsed = time.time() - start_time
    
    print(f"\nâœ… ML í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ (âš¡ {elapsed:.2f}ì´ˆ)")
    print(f"   ìƒì„±ëœ ê·¸ë£¹ ìˆ˜: {len(color_groups)}ê°œ")
    for color_name in color_order:
        if color_name in color_groups:
            count = len(color_groups[color_name])
            avg_conf = np.mean([h["clip_confidence"] for h in color_groups[color_name]])
            print(f"   {color_name}: {count}ê°œ í™€ë“œ (í‰ê·  ì‹ ë¢°ë„: {avg_conf:.2f})")
    
    return hold_data


def train_all_models():
    """ëª¨ë“  ëª¨ë¸ í•™ìŠµ (KNN, SVM, NN)"""
    print("=" * 60)
    print("ğŸ¤– ê²½ëŸ‰ ìƒ‰ìƒ ë¶„ë¥˜ AI ëª¨ë¸ í•™ìŠµ")
    print("=" * 60)
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    dataset = load_feedback_dataset()
    if dataset is None:
        return
    
    if dataset['total_samples'] < 10:
        print(f"âš ï¸ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤ ({dataset['total_samples']}ê°œ)")
        print("   ìµœì†Œ 30ê°œ ì´ìƒì˜ í”¼ë“œë°±ì„ ìˆ˜ì§‘í•˜ì„¸ìš”!")
        return
    
    # ë°ì´í„° ì¤€ë¹„
    X, y = prepare_training_data(dataset)
    print(f"\nğŸ“Š í•™ìŠµ ë°ì´í„°: {len(X)}ê°œ ìƒ˜í”Œ, {len(np.unique(y))}ê°œ í´ë˜ìŠ¤")
    print(f"   í´ë˜ìŠ¤: {np.unique(y)}")
    
    # 1. KNN í•™ìŠµ
    try:
        knn_model, knn_scaler = train_knn_model(X, y)
        save_model(knn_model, knn_scaler, "knn")
    except Exception as e:
        print(f"âŒ KNN í•™ìŠµ ì‹¤íŒ¨: {e}")
    
    # 2. SVM í•™ìŠµ
    try:
        svm_model, svm_scaler = train_svm_model(X, y)
        save_model(svm_model, svm_scaler, "svm")
    except Exception as e:
        print(f"âŒ SVM í•™ìŠµ ì‹¤íŒ¨: {e}")
    
    # 3. ì‹ ê²½ë§ í•™ìŠµ (TensorFlow ìˆìœ¼ë©´)
    try:
        nn_model, nn_scaler, nn_labels = train_neural_network_model(X, y)
        save_model(nn_model, nn_scaler, "neural_network", nn_labels)
    except ImportError:
        print("âš ï¸ TensorFlow ë¯¸ì„¤ì¹˜ - ì‹ ê²½ë§ í•™ìŠµ ê±´ë„ˆëœ€")
    except Exception as e:
        print(f"âŒ ì‹ ê²½ë§ í•™ìŠµ ì‹¤íŒ¨: {e}")
    
    print("\n" + "=" * 60)
    print("âœ… ëª¨ë“  ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    print("=" * 60)
    print("\nğŸ’¡ ì‚¬ìš© ë°©ë²•:")
    print("   from train_color_classifier import ml_based_color_clustering")
    print("   hold_data = ml_based_color_clustering(hold_data, None, model_type='knn')")


def evaluate_models():
    """í•™ìŠµëœ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, classification_report
    
    print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
    print("=" * 60)
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    dataset = load_feedback_dataset()
    if dataset is None:
        return
    
    X, y = prepare_training_data(dataset)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # ê° ëª¨ë¸ í‰ê°€
    for model_type in ["knn", "svm"]:
        try:
            model, scaler, _ = load_trained_model(model_type)
            
            # ì˜ˆì¸¡
            X_test_scaled = scaler.transform(X_test)
            y_pred = model.predict(X_test_scaled)
            
            # ì •í™•ë„
            accuracy = accuracy_score(y_test, y_pred)
            
            print(f"\n{model_type.upper()} ëª¨ë¸:")
            print(f"  ì •í™•ë„: {accuracy:.1%}")
            print("\n" + classification_report(y_test, y_pred))
        
        except Exception as e:
            print(f"âŒ {model_type.upper()} í‰ê°€ ì‹¤íŒ¨: {e}")
    
    print("=" * 60)


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "evaluate":
        evaluate_models()
    else:
        train_all_models()

