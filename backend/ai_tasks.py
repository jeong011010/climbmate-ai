import os
import cv2
import numpy as np
import base64
import json
from celery import current_task
from celery_app import celery_app
from holdcheck import preprocess, clustering
from backend.gpt4_analyzer import analyze_with_gpt4_vision

@celery_app.task(bind=True)
def analyze_colors_with_clip_async(self, image_base64, hold_data):
    """
    CLIP ìƒ‰ìƒ ë¶„ì„ ë¹„ë™ê¸° ì‘ì—…
    
    Args:
        image_base64: Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë°ì´í„°
        hold_data: í™€ë“œ ê°ì§€ ê²°ê³¼
    
    Returns:
        list: ìƒ‰ìƒ ë¶„ì„ëœ í™€ë“œ ë°ì´í„°
    """
    try:
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: ì´ë¯¸ì§€ ë””ì½”ë”©
        self.update_state(
            state='PROGRESS',
            meta={'progress': 10, 'message': 'ğŸ“¸ ì´ë¯¸ì§€ ë””ì½”ë”© ì¤‘...', 'step': 'decode'}
        )
        
        # Base64 ì´ë¯¸ì§€ ë””ì½”ë”©
        image_bytes = base64.b64decode(image_base64)
        nparr = np.frombuffer(image_bytes, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if image is None:
            raise ValueError("ì˜ëª»ëœ ì´ë¯¸ì§€ íŒŒì¼")
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: CLIP ë¶„ì„ ì‹œì‘
        self.update_state(
            state='PROGRESS',
            meta={'progress': 30, 'message': 'ğŸ¨ CLIP ìƒ‰ìƒ ë¶„ì„ ì¤‘...', 'step': 'clip_analysis'}
        )
        
        # í™€ë“œ ë°ì´í„°ë¥¼ ë§ˆìŠ¤í¬ë¡œ ë³€í™˜ (ê°„ë‹¨í•œ êµ¬í˜„)
        masks = []
        for hold in hold_data:
            # í™€ë“œ ì¤‘ì‹¬ì  ì£¼ë³€ì„ ë§ˆìŠ¤í¬ë¡œ ìƒì„±
            mask = np.zeros(image.shape[:2], dtype=np.uint8)
            center_x, center_y = int(hold['center'][0]), int(hold['center'][1])
            radius = int(np.sqrt(hold['area']) / 2)
            cv2.circle(mask, (center_x, center_y), radius, 255, -1)
            masks.append(mask)
        
        # CLIP ìƒ‰ìƒ ë¶„ì„
        colored_holds = clustering.clip_ai_color_clustering(
            hold_data,
            None,
            image,
            masks,
            eps=0.3,
            use_dbscan=False
        )
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: ì™„ë£Œ
        self.update_state(
            state='SUCCESS',
            meta={
                'progress': 100, 
                'message': 'âœ… CLIP ìƒ‰ìƒ ë¶„ì„ ì™„ë£Œ', 
                'step': 'complete',
                'result': colored_holds
            }
        )
        
        return colored_holds
        
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
        self.update_state(
            state='FAILURE',
            meta={
                'progress': 0,
                'message': f'âŒ CLIP ë¶„ì„ ì‹¤íŒ¨: {str(e)}',
                'step': 'error',
                'error': str(e)
            }
        )
        raise e
    """
    ë¹„ë™ê¸° ì´ë¯¸ì§€ ë¶„ì„ ì‘ì—…
    
    Args:
        image_data: Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë°ì´í„°
        wall_angle: ë²½ ê°ë„ (overhang, slab, face, null)
    
    Returns:
        dict: ë¶„ì„ ê²°ê³¼
    """
    try:
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: ì´ë¯¸ì§€ ë””ì½”ë”©
        self.update_state(
            state='PROGRESS',
            meta={'progress': 5, 'message': 'ğŸ“¸ ì´ë¯¸ì§€ ë””ì½”ë”© ì¤‘...', 'step': 'decode'}
        )
        
        # Base64 ì´ë¯¸ì§€ ë””ì½”ë”©
        image_bytes = base64.b64decode(image_data)
        nparr = np.frombuffer(image_bytes, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if image is None:
            raise ValueError("ì˜ëª»ëœ ì´ë¯¸ì§€ íŒŒì¼")
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: í™€ë“œ ê°ì§€ ì‹œì‘
        self.update_state(
            state='PROGRESS',
            meta={'progress': 10, 'message': 'ğŸ” í™€ë“œ ê°ì§€ ì¤‘...', 'step': 'detection'}
        )
        
        # ğŸš€ ìµœì í™”: ì „ì²˜ë¦¬ (í™€ë“œ ê°ì§€)
        model_path = "/app/holdcheck/roboflow_weights/weights.pt" if os.path.exists("/app/holdcheck/roboflow_weights/weights.pt") else "/Users/kimjazz/Desktop/project/climbmate/holdcheck/roboflow_weights/weights.pt"
        
        hold_data_raw, masks = preprocess(
            image,
            model_path=model_path,
            mask_refinement=1,
            conf=0.4,
            use_clip_ai=True
        )
        
        if not hold_data_raw:
            raise ValueError("í™€ë“œë¥¼ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: í™€ë“œ ê°ì§€ ì™„ë£Œ
        self.update_state(
            state='PROGRESS',
            meta={
                'progress': 30, 
                'message': f'âœ… {len(hold_data_raw)}ê°œ í™€ë“œ ê°ì§€ ì™„ë£Œ', 
                'step': 'detection_complete',
                'holds_count': len(hold_data_raw)
            }
        )
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: ìƒ‰ìƒ ê·¸ë£¹í•‘
        self.update_state(
            state='PROGRESS',
            meta={'progress': 40, 'message': 'ğŸ¨ ìƒ‰ìƒ ë¶„ë¥˜ ì¤‘...', 'step': 'clustering'}
        )
        
        # ìƒ‰ìƒ ê·¸ë£¹í•‘
        hold_data = clustering.clip_ai_color_clustering(
            hold_data_raw,
            None,
            image,
            masks,
            eps=0.3,
            use_dbscan=False
        )
        
        # ë¬¸ì œ ê·¸ë£¹í•‘
        problems = clustering.group_holds_by_color(hold_data)
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: ìƒ‰ìƒ ë¶„ë¥˜ ì™„ë£Œ
        self.update_state(
            state='PROGRESS',
            meta={
                'progress': 60, 
                'message': f'âœ… {len(problems)}ê°œ ë¬¸ì œ ë¶„ë¥˜ ì™„ë£Œ', 
                'step': 'clustering_complete',
                'problems_count': len(problems)
            }
        )
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: AI ë¬¸ì œ ë¶„ì„ ì‹œì‘
        self.update_state(
            state='PROGRESS',
            meta={'progress': 70, 'message': 'ğŸ¤– AI ë¬¸ì œ ë¶„ì„ ì¤‘...', 'step': 'analysis'}
        )
        
        # GPT-4 ë¶„ì„
        problems_with_analysis = {}
        for color, holds in problems.items():
            if len(holds) >= 3:  # ìµœì†Œ 3ê°œ í™€ë“œ ì´ìƒì¸ ë¬¸ì œë§Œ ë¶„ì„
                try:
                    # ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©
                    _, buffer = cv2.imencode('.jpg', image)
                    image_base64 = base64.b64encode(buffer).decode('utf-8')
                    
                    # GPT-4 ë¶„ì„
                    analysis = analyze_with_gpt4_vision(image_base64, holds, wall_angle)
                    
                    problems_with_analysis[color] = {
                        'color_name': color,
                        'color_rgb': holds[0].get('dominant_rgb', [128, 128, 128]),
                        'holds': holds,
                        'hold_count': len(holds),
                        'analysis': analysis
                    }
                except Exception as e:
                    print(f"GPT-4 ë¶„ì„ ì‹¤íŒ¨ ({color}): {e}")
                    problems_with_analysis[color] = {
                        'color_name': color,
                        'color_rgb': holds[0].get('dominant_rgb', [128, 128, 128]),
                        'holds': holds,
                        'hold_count': len(holds),
                        'analysis': None
                    }
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: AI ë¶„ì„ ì™„ë£Œ
        self.update_state(
            state='PROGRESS',
            meta={'progress': 90, 'message': 'âœ… AI ë¶„ì„ ì™„ë£Œ', 'step': 'analysis_complete'}
        )
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: ê²°ê³¼ ì •ë¦¬
        self.update_state(
            state='PROGRESS',
            meta={'progress': 95, 'message': 'ğŸ“Š ê²°ê³¼ ì •ë¦¬ ì¤‘...', 'step': 'finalizing'}
        )
        
        # ìµœì¢… ê²°ê³¼ êµ¬ì„±
        problems_list = list(problems_with_analysis.values())
        
        # í†µê³„ ê³„ì‚°
        total_holds = len(hold_data_raw)
        total_problems = len(problems_list)
        analyzable_problems = len([p for p in problems_list if p and p.get('hold_count', 0) >= 3])
        
        statistics = {
            "total_holds": total_holds,
            "total_problems": total_problems,
            "analyzable_problems": analyzable_problems
        }
        
        # ì£¼ì„ ë‹¬ë¦° ì´ë¯¸ì§€ ìƒì„±
        annotated_image = None
        if masks is not None:
            try:
                overlay = image.copy()
                colors = [
                    (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0),
                    (255, 0, 255), (0, 255, 255), (128, 0, 0), (0, 128, 0),
                    (0, 0, 128), (128, 128, 0), (128, 0, 128), (0, 128, 128)
                ]
                
                for i, mask in enumerate(masks):
                    if i < len(colors):
                        color = colors[i % len(colors)]
                        overlay[mask > 0.5] = color
                
                annotated = cv2.addWeighted(image, 0.7, overlay, 0.3, 0)
                _, buffer = cv2.imencode('.jpg', annotated)
                annotated_image = base64.b64encode(buffer).decode('utf-8')
            except Exception as e:
                print(f"âš ï¸ ì£¼ì„ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
        
        # ìµœì¢… ê²°ê³¼
        result = {
            "problems": problems_list,
            "statistics": statistics,
            "hold_data": hold_data,
            "annotated_image_base64": annotated_image
        }
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: ì™„ë£Œ
        self.update_state(
            state='SUCCESS',
            meta={
                'progress': 100, 
                'message': 'âœ… ë¶„ì„ ì™„ë£Œ!', 
                'step': 'complete',
                'result': result
            }
        )
        
        return result
        
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
        self.update_state(
            state='FAILURE',
            meta={
                'progress': 0,
                'message': f'âŒ ë¶„ì„ ì‹¤íŒ¨: {str(e)}',
                'step': 'error',
                'error': str(e)
            }
        )
        raise e
