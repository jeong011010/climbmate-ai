"""
ìì²´ ML ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸
ê²€ì¦ëœ ë°ì´í„° 50ê°œ ì´ìƒë¶€í„° í•™ìŠµ ê°€ëŠ¥
"""

import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder
import pickle
import os
from typing import List, Dict, Tuple

MODEL_DIR = os.path.join(os.path.dirname(__file__), 'models')
os.makedirs(MODEL_DIR, exist_ok=True)

DIFFICULTY_MODEL_PATH = os.path.join(MODEL_DIR, 'difficulty_model.pkl')
TYPE_MODEL_PATH = os.path.join(MODEL_DIR, 'type_model.pkl')
DIFFICULTY_ENCODER_PATH = os.path.join(MODEL_DIR, 'difficulty_encoder.pkl')
TYPE_ENCODER_PATH = os.path.join(MODEL_DIR, 'type_encoder.pkl')

# ğŸ¨ ìƒ‰ìƒ ë¶„ë¥˜ ëª¨ë¸ ê²½ë¡œ
COLOR_MODEL_PATH = os.path.join(MODEL_DIR, 'color_model.pkl')
COLOR_ENCODER_PATH = os.path.join(MODEL_DIR, 'color_encoder.pkl')

def extract_features(holds_data: List[Dict], stats: Dict = None) -> np.ndarray:
    """í™€ë“œ ë°ì´í„°ë¡œë¶€í„° íŠ¹ì§• ë²¡í„° ì¶”ì¶œ"""
    
    num_holds = len(holds_data)
    
    # í™€ë“œ í¬ê¸° í†µê³„
    areas = [h.get('area', 0) for h in holds_data]
    avg_area = np.mean(areas)
    min_area = np.min(areas)
    max_area = np.max(areas)
    std_area = np.std(areas)
    
    # í™€ë“œ ìœ„ì¹˜ í†µê³„
    centers = np.array([h.get('center', [0, 0]) for h in holds_data])
    
    # ê±°ë¦¬ ê³„ì‚°
    distances = []
    consecutive_distances = []
    if num_holds > 1:
        for i in range(len(centers)):
            for j in range(i+1, len(centers)):
                dist = np.linalg.norm(centers[i] - centers[j])
                distances.append(dist)
        
        # ë†’ì´ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì—°ì† ê±°ë¦¬
        sorted_indices = np.argsort(centers[:, 1])[::-1]
        for i in range(len(sorted_indices) - 1):
            dist = np.linalg.norm(
                centers[sorted_indices[i]] - centers[sorted_indices[i+1]]
            )
            consecutive_distances.append(dist)
    
    max_distance = max(distances) if distances else 0
    avg_distance = np.mean(distances) if distances else 0
    avg_consecutive = np.mean(consecutive_distances) if consecutive_distances else 0
    
    # ë†’ì´/ìˆ˜í‰ ë²”ìœ„
    heights = centers[:, 1]
    horizontals = centers[:, 0]
    height_range = np.ptp(heights) if num_holds > 1 else 0
    horizontal_range = np.ptp(horizontals) if num_holds > 1 else 0
    
    # ìˆ˜í‰/ìˆ˜ì§ ë¹„ìœ¨
    movement_ratio = horizontal_range / (height_range + 1)
    
    # ìƒ‰ìƒ ë¶„í¬
    colors = [h.get('color_name', 'unknown') for h in holds_data]
    unique_colors = len(set(colors))
    
    # íŠ¹ì§• ë²¡í„° (25ê°œ íŠ¹ì§•)
    features = [
        num_holds,              # 1. í™€ë“œ ê°œìˆ˜
        avg_area,               # 2. í‰ê·  í™€ë“œ í¬ê¸°
        min_area,               # 3. ìµœì†Œ í™€ë“œ í¬ê¸°
        max_area,               # 4. ìµœëŒ€ í™€ë“œ í¬ê¸°
        std_area,               # 5. í™€ë“œ í¬ê¸° ë¶„ì‚°
        max_distance,           # 6. ìµœëŒ€ í™€ë“œ ê°„ê²©
        avg_distance,           # 7. í‰ê·  í™€ë“œ ê°„ê²©
        avg_consecutive,        # 8. ì—°ì† í™€ë“œ í‰ê·  ê°„ê²©
        height_range,           # 9. ë†’ì´ ë²”ìœ„
        horizontal_range,       # 10. ìˆ˜í‰ ë²”ìœ„
        movement_ratio,         # 11. ì´ë™ ë¹„ìœ¨ (ìˆ˜í‰/ìˆ˜ì§)
        unique_colors,          # 12. ê³ ìœ  ìƒ‰ìƒ ìˆ˜
        # ë¹„ìœ¨ íŠ¹ì§•
        len([a for a in areas if a < 1200]) / num_holds,  # 13. ì‘ì€ í™€ë“œ ë¹„ìœ¨
        len([a for a in areas if a > 3500]) / num_holds,  # 14. í° í™€ë“œ ë¹„ìœ¨
        # ë¶„í¬ íŠ¹ì§•
        np.std(centers[:, 0]) if num_holds > 1 else 0,    # 15. ìˆ˜í‰ ë¶„ì‚°
        np.std(centers[:, 1]) if num_holds > 1 else 0,    # 16. ìˆ˜ì§ ë¶„ì‚°
        # ê±°ë¦¬ ë¶„ì‚°
        np.std(distances) if distances else 0,            # 17. ê±°ë¦¬ ë¶„ì‚°
        # ë°€ë„
        num_holds / (height_range * horizontal_range + 1),  # 18. í™€ë“œ ë°€ë„
        # í‰ê·  ìœ„ì¹˜
        np.mean(centers[:, 0]),                           # 19. í‰ê·  X ìœ„ì¹˜
        np.mean(centers[:, 1]),                           # 20. í‰ê·  Y ìœ„ì¹˜
        # ìµœìƒë‹¨/ìµœí•˜ë‹¨ ê±°ë¦¬
        np.max(heights) - np.min(heights) if num_holds > 1 else 0,  # 21. ë†’ì´ ë³€í™”
        # ì—°ì† ê±°ë¦¬ ë¶„ì‚°
        np.std(consecutive_distances) if consecutive_distances else 0,  # 22. ì—°ì† ê±°ë¦¬ ë¶„ì‚°
        # í™€ë“œ í¬ê¸° ë²”ìœ„
        max_area - min_area,                              # 23. í¬ê¸° ë²”ìœ„
        # ê·¹ë‹¨ê°’ ë¹„ìœ¨
        len([d for d in distances if d > 150]) / len(distances) if distances else 0,  # 24. í° ì í”„ ë¹„ìœ¨
        len([a for a in areas if a < 1000]) / num_holds   # 25. ê·¹ì†Œí˜• í™€ë“œ ë¹„ìœ¨
    ]
    
    return np.array(features)

def train_difficulty_model(training_data: List[Dict]) -> Tuple[float, float]:
    """ë‚œì´ë„ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ"""
    
    print(f"\nğŸ“ ë‚œì´ë„ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    print(f"   í›ˆë ¨ ë°ì´í„°: {len(training_data)}ê°œ")
    
    # íŠ¹ì§• ì¶”ì¶œ
    X = []
    y = []
    
    for data in training_data:
        features = extract_features(data['holds_data'])
        X.append(features)
        y.append(data['difficulty'])
    
    X = np.array(X)
    y = np.array(y)
    
    # ë¼ë²¨ ì¸ì½”ë”©
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    
    # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=0.2, random_state=42
    )
    
    # ëª¨ë¸ í•™ìŠµ (Gradient Boosting - ë” ì •í™•)
    model = GradientBoostingClassifier(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=5,
        random_state=42
    )
    
    model.fit(X_train, y_train)
    
    # ì •í™•ë„ í‰ê°€
    train_accuracy = model.score(X_train, y_train)
    test_accuracy = model.score(X_test, y_test)
    
    # Cross-validation
    cv_scores = cross_val_score(model, X, y_encoded, cv=min(5, len(X)))
    cv_accuracy = np.mean(cv_scores)
    
    print(f"   âœ… í›ˆë ¨ ì •í™•ë„: {train_accuracy*100:.1f}%")
    print(f"   âœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy*100:.1f}%")
    print(f"   âœ… CV ì •í™•ë„: {cv_accuracy*100:.1f}%")
    
    # ëª¨ë¸ ì €ì¥
    with open(DIFFICULTY_MODEL_PATH, 'wb') as f:
        pickle.dump(model, f)
    with open(DIFFICULTY_ENCODER_PATH, 'wb') as f:
        pickle.dump(label_encoder, f)
    
    print(f"   ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {DIFFICULTY_MODEL_PATH}")
    
    return test_accuracy, cv_accuracy

def train_type_model(training_data: List[Dict]) -> Tuple[float, float]:
    """ìœ í˜• ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ"""
    
    print(f"\nğŸ“ ìœ í˜• ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    print(f"   í›ˆë ¨ ë°ì´í„°: {len(training_data)}ê°œ")
    
    # íŠ¹ì§• ì¶”ì¶œ
    X = []
    y = []
    
    for data in training_data:
        features = extract_features(data['holds_data'])
        X.append(features)
        y.append(data['type'])
    
    X = np.array(X)
    y = np.array(y)
    
    # ë¼ë²¨ ì¸ì½”ë”©
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    
    # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=0.2, random_state=42
    )
    
    # ëª¨ë¸ í•™ìŠµ
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42
    )
    
    model.fit(X_train, y_train)
    
    # ì •í™•ë„ í‰ê°€
    train_accuracy = model.score(X_train, y_train)
    test_accuracy = model.score(X_test, y_test)
    
    # Cross-validation
    cv_scores = cross_val_score(model, X, y_encoded, cv=min(5, len(X)))
    cv_accuracy = np.mean(cv_scores)
    
    print(f"   âœ… í›ˆë ¨ ì •í™•ë„: {train_accuracy*100:.1f}%")
    print(f"   âœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy*100:.1f}%")
    print(f"   âœ… CV ì •í™•ë„: {cv_accuracy*100:.1f}%")
    
    # ëª¨ë¸ ì €ì¥
    with open(TYPE_MODEL_PATH, 'wb') as f:
        pickle.dump(model, f)
    with open(TYPE_ENCODER_PATH, 'wb') as f:
        pickle.dump(label_encoder, f)
    
    print(f"   ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {TYPE_MODEL_PATH}")
    
    return test_accuracy, cv_accuracy

def predict_difficulty(holds_data: List[Dict]) -> Dict:
    """í•™ìŠµëœ ëª¨ë¸ë¡œ ë‚œì´ë„ ì˜ˆì¸¡"""
    
    if not os.path.exists(DIFFICULTY_MODEL_PATH):
        return {'grade': None, 'confidence': 0.0, 'available': False}
    
    try:
        # ëª¨ë¸ ë¡œë“œ
        with open(DIFFICULTY_MODEL_PATH, 'rb') as f:
            model = pickle.load(f)
        with open(DIFFICULTY_ENCODER_PATH, 'rb') as f:
            encoder = pickle.load(f)
        
        # íŠ¹ì§• ì¶”ì¶œ
        features = extract_features(holds_data)
        features = features.reshape(1, -1)
        
        # ì˜ˆì¸¡
        prediction = model.predict(features)[0]
        probabilities = model.predict_proba(features)[0]
        
        grade = encoder.inverse_transform([prediction])[0]
        confidence = float(np.max(probabilities))
        
        return {
            'grade': grade,
            'confidence': confidence,
            'available': True
        }
    except Exception as e:
        print(f"âš ï¸ ë‚œì´ë„ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        return {'grade': None, 'confidence': 0.0, 'available': False}

def predict_type(holds_data: List[Dict]) -> Dict:
    """í•™ìŠµëœ ëª¨ë¸ë¡œ ìœ í˜• ì˜ˆì¸¡"""
    
    if not os.path.exists(TYPE_MODEL_PATH):
        return {'type': None, 'confidence': 0.0, 'available': False}
    
    try:
        # ëª¨ë¸ ë¡œë“œ
        with open(TYPE_MODEL_PATH, 'rb') as f:
            model = pickle.load(f)
        with open(TYPE_ENCODER_PATH, 'rb') as f:
            encoder = pickle.load(f)
        
        # íŠ¹ì§• ì¶”ì¶œ
        features = extract_features(holds_data)
        features = features.reshape(1, -1)
        
        # ì˜ˆì¸¡
        prediction = model.predict(features)[0]
        probabilities = model.predict_proba(features)[0]
        
        climb_type = encoder.inverse_transform([prediction])[0]
        confidence = float(np.max(probabilities))
        
        return {
            'type': climb_type,
            'confidence': confidence,
            'available': True
        }
    except Exception as e:
        print(f"âš ï¸ ìœ í˜• ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        return {'type': None, 'confidence': 0.0, 'available': False}

def get_model_availability() -> Dict:
    """ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    return {
        'difficulty_model': os.path.exists(DIFFICULTY_MODEL_PATH),
        'type_model': os.path.exists(TYPE_MODEL_PATH),
        'color_model': os.path.exists(COLOR_MODEL_PATH)
    }

# ğŸ¨ ===== ìƒ‰ìƒ ë¶„ë¥˜ ëª¨ë¸ ===== ğŸ¨

def extract_color_features(color_data: Dict) -> np.ndarray:
    """ğŸ¨ í™€ë“œ ìƒ‰ìƒ ë°ì´í„°ë¡œë¶€í„° íŠ¹ì§• ë²¡í„° ì¶”ì¶œ"""
    
    rgb = color_data.get('rgb', [128, 128, 128])
    hsv = color_data.get('hsv', [0, 0, 128])
    lab = color_data.get('lab', [0, 0, 0])
    
    # ê¸°ë³¸ ìƒ‰ìƒ íŠ¹ì§• (9ê°œ)
    features = [
        rgb[0], rgb[1], rgb[2],  # RGB
        hsv[0], hsv[1], hsv[2],  # HSV
        lab[0], lab[1], lab[2],  # LAB
    ]
    
    # í†µê³„ íŠ¹ì§• ì¶”ê°€ (color_statsì—ì„œ)
    color_stats = color_data.get('color_stats', {})
    
    # HSV í†µê³„
    hsv_stats = color_stats.get('hsv_stats', {})
    features.extend([
        hsv_stats.get('mean', [0, 0, 0])[0],  # H í‰ê· 
        hsv_stats.get('mean', [0, 0, 0])[1],  # S í‰ê· 
        hsv_stats.get('mean', [0, 0, 0])[2],  # V í‰ê· 
        hsv_stats.get('std', [0, 0, 0])[1],   # S í‘œì¤€í¸ì°¨
        hsv_stats.get('std', [0, 0, 0])[2],   # V í‘œì¤€í¸ì°¨
    ])
    
    # RGB í†µê³„
    rgb_stats = color_stats.get('rgb_stats', {})
    features.extend([
        rgb_stats.get('std', [0, 0, 0])[0],   # R í‘œì¤€í¸ì°¨
        rgb_stats.get('std', [0, 0, 0])[1],   # G í‘œì¤€í¸ì°¨
        rgb_stats.get('std', [0, 0, 0])[2],   # B í‘œì¤€í¸ì°¨
    ])
    
    # LAB í†µê³„
    lab_stats = color_stats.get('lab_stats', {})
    features.extend([
        lab_stats.get('mean', [0, 0, 0])[1],  # a í‰ê·  (ë¹¨ê°•-ë…¹ìƒ‰)
        lab_stats.get('mean', [0, 0, 0])[2],  # b í‰ê·  (íŒŒë‘-ë…¸ë‘)
    ])
    
    # ì¶”ê°€ íŠ¹ì§•
    features.extend([
        color_data.get('area', 0) / 10000,     # í™€ë“œ í¬ê¸° (ì •ê·œí™”)
        color_data.get('circularity', 0)       # í™€ë“œ ì›í˜•ë„
    ])
    
    return np.array(features)

def train_color_model(training_data: List[Dict]) -> Tuple[float, float]:
    """ğŸ¨ ìƒ‰ìƒ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ"""
    
    print(f"\nğŸ¨ ìƒ‰ìƒ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    print(f"   í›ˆë ¨ ë°ì´í„°: {len(training_data)}ê°œ")
    
    if len(training_data) < 10:
        print(f"   âš ï¸ ë°ì´í„° ë¶€ì¡±! ìµœì†Œ 10ê°œ í•„ìš” (í˜„ì¬: {len(training_data)}ê°œ)")
        return 0.0, 0.0
    
    # íŠ¹ì§• ì¶”ì¶œ
    X = []
    y = []
    
    for data in training_data:
        try:
            features = extract_color_features(data)
            X.append(features)
            y.append(data['correct_color'])
        except Exception as e:
            print(f"   âš ï¸ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            continue
    
    if len(X) < 10:
        print(f"   âš ï¸ ìœ íš¨ ë°ì´í„° ë¶€ì¡±! (í˜„ì¬: {len(X)}ê°œ)")
        return 0.0, 0.0
    
    X = np.array(X)
    y = np.array(y)
    
    print(f"   ìƒ‰ìƒ ë¶„í¬: {np.unique(y, return_counts=True)}")
    
    # ë¼ë²¨ ì¸ì½”ë”©
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    
    # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    
    # ëª¨ë¸ í•™ìŠµ (Random Forest - ìƒ‰ìƒ ë¶„ë¥˜ì— ì í•©)
    model = RandomForestClassifier(
        n_estimators=200,
        max_depth=15,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        class_weight='balanced'  # ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬
    )
    
    model.fit(X_train, y_train)
    
    # ì •í™•ë„ í‰ê°€
    train_accuracy = model.score(X_train, y_train)
    test_accuracy = model.score(X_test, y_test)
    
    # Cross-validation
    cv_scores = cross_val_score(model, X, y_encoded, cv=min(5, len(X)//3))
    cv_accuracy = np.mean(cv_scores)
    
    print(f"   âœ… í›ˆë ¨ ì •í™•ë„: {train_accuracy*100:.1f}%")
    print(f"   âœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy*100:.1f}%")
    print(f"   âœ… CV ì •í™•ë„: {cv_accuracy*100:.1f}%")
    
    # Feature Importance
    feature_importance = model.feature_importances_
    top_features = np.argsort(feature_importance)[::-1][:5]
    print(f"   ğŸ” ì¤‘ìš” íŠ¹ì§• (ì¸ë±ìŠ¤): {top_features}")
    
    # ëª¨ë¸ ì €ì¥
    with open(COLOR_MODEL_PATH, 'wb') as f:
        pickle.dump(model, f)
    with open(COLOR_ENCODER_PATH, 'wb') as f:
        pickle.dump(label_encoder, f)
    
    print(f"   ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {COLOR_MODEL_PATH}")
    
    return test_accuracy, cv_accuracy

def predict_color(hold_features: Dict) -> Dict:
    """ğŸ¨ í•™ìŠµëœ ëª¨ë¸ë¡œ í™€ë“œ ìƒ‰ìƒ ì˜ˆì¸¡"""
    
    if not os.path.exists(COLOR_MODEL_PATH):
        return {'color': None, 'confidence': 0.0, 'available': False}
    
    try:
        # ëª¨ë¸ ë¡œë“œ
        with open(COLOR_MODEL_PATH, 'rb') as f:
            model = pickle.load(f)
        with open(COLOR_ENCODER_PATH, 'rb') as f:
            encoder = pickle.load(f)
        
        # íŠ¹ì§• ì¶”ì¶œ
        features = extract_color_features(hold_features)
        features = features.reshape(1, -1)
        
        # ì˜ˆì¸¡
        prediction = model.predict(features)[0]
        probabilities = model.predict_proba(features)[0]
        
        color = encoder.inverse_transform([prediction])[0]
        confidence = float(np.max(probabilities))
        
        # ìƒìœ„ 3ê°œ ì˜ˆì¸¡
        top_3_idx = np.argsort(probabilities)[::-1][:3]
        top_3_colors = encoder.inverse_transform(top_3_idx)
        top_3_probs = probabilities[top_3_idx]
        
        return {
            'color': color,
            'confidence': confidence,
            'available': True,
            'top_3': list(zip(top_3_colors, top_3_probs.tolist()))
        }
    except Exception as e:
        print(f"âš ï¸ ìƒ‰ìƒ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        return {'color': None, 'confidence': 0.0, 'available': False}

